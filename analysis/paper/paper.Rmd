---
title: "Playing around with ogham stones in R"
author:
  - Schmidt, Sophie
  - Thiery, Florian
  - Homburg, Timo
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---


<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)
```

# Introduction

It is the aim of this package to use the data of the Ogi-Ogham project to create some maps and geostatistics with R.


```{r spatial data}
# load data
ogham <- read.csv2("../data/raw_data/2020-01-10_townlandsxy.csv", sep = "\t")

library(tidyr)

ogha <- separate(ogham, latlon, c("lat", "lon"), ';')

# lat lon were saved as characters, make them numbers again
ogha$lat <- as.numeric(as.character(ogha$lat))
ogha$lon <- as.numeric(as.character(ogha$lon))


##create spatialpointsdataframe
library(sp)

# transformation into Easting and Northing for percolation
coordinates(ogha) <- ~lon+lat
proj4string(ogha) <- CRS("+proj=longlat + ellps=WGS84") # This sets the projection of the data, assuming that you are using the WGS84 ellipsoid (if you do not know for sure, it is a safe assumption)

data <-spTransform(ogha,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.


ogha <- as.data.frame(coordinates(data)) # This converts the SpatialPoints object back into a data frame


# load shape
library(rgdal)

bg <- readOGR(dsn = "../data/raw_data",layer = "ne_10m_admin_0_countries")

gb_ie <- subset(bg, bg@data$ADMIN == 'United Kingdom' | bg@data$ADMIN == 'Ireland')

ie <- subset(bg, bg@data$ADMIN == 'Ireland')

gb_ie <- spTransform(gb_ie,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.


```


## Percolation is an explorative clustering algorithm


```{r percolation}

#devtools::install_github("scschmidt/percopackage")
library("percopackage")

ogg <- cbind(ogham$ciic, ogha)

## the algorithm had problems with IDs with a "-"  -- therefore they needed to be renamed to just the first number
ogg_renamed <- ogg

library(tidyr)
# separating the IDs into two columns
ogg_renamed <- separate(data = ogg_renamed, col = PlcIndex, into = c("left", "right"), sep = "-")

#naming the columns
colnames(ogg_renamed) <- c("PlcIndex", "weg", "Easting", "Northing")

# deleting the not needed column
ogg_renamed <- ogg_renamed[,-2]

# there's a mistake there somewhere
ogg_renamed$PlcIndex[ogg_renamed$PlcIndex == "'12"] <- "12"

# percolation
percolate(data = ogg_renamed, distance_table = NULL,limit = 100, radius_unit = 1000, upper_radius = 60, lower_radius = 1, step_value = 2)

#transformation of map to same PRJ
gb_ie <- spTransform(gb_ie,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.

# mapping of percolation
mapClusters(shape = gb_ie, map_name = "ogham stones in Ireland", source_file_name = "after MacAlister 1945", dpi = 300)

# create frequency plots
plotClustFreq()


```


## density Karte

```{r density plot}

library(ggplot2)
library(ggspatial)

ggplot()+
    stat_density_2d(data = ogha, aes(x = lon, y = lat, fill = ..level..), alpha = 0.5, h = ogha$count, geom = "polygon")+
    scale_fill_distiller(palette="Spectral", direction=-1,
                         name = "Level of density") +
    geom_polygon(data = gb_ie, aes(x = long, y = lat, group = group), fill = "white", alpha = 0.1, colour = "lightgrey", size = 0.5) +
    annotation_scale(location = "bl", width_hint = 0.2) +
    annotation_north_arrow(location = "bl", which_north = "true", 
                          pad_x = unit(0.1, "in"), pad_y = unit(0.2, "in"),
                          style = north_arrow_fancy_orienteering) +
    geom_point(data = ogha, aes(x = lon, y = lat), size = 0.5) +
    coord_equal(ylim=c(5500000,6250000), xlim = c(250000, 800000))+
    theme_classic()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))+
    scale_x_continuous(labels = scales::comma)+
    scale_y_continuous(labels = scales::comma)

ggsave("../figures/densitymap.jpg", width = 20, height = 18, units = "cm", dpi = 300)

```




# wie häufig welches Wort


```{r load word data}

ciic_words <- read.csv2("../data/raw_data/ciictowords2.csv", sep = "|")
ciic_words <- ciic_words[,-c(2:5)]
ciic_words[is.na(ciic_words)] <- 0


words_ciic <- t(as.matrix(ciic_words)) # transpose = switch columns and rows
words_ciic <- as.data.frame(words_ciic)

# ciic numbers into numbers again for column heads

for (i in 1:323) {
  words_ciic[,i] <- as.numeric(as.character(words_ciic[,i]))
}

# column heads to be ciic
colnames(words_ciic) <- ciic_words$ciic

#delete ciic from table
words_ciic <- words_ciic[-1,]



```

## Datensatz säubern
Da sowohl Varianten von Wörtern als auch die "eigentlichen" Wörter in der Tabelle aufgenommen wurden, muss der Datensatz bereinigt werden (ansonsten sehr hohe Korrelationen von "eigentlichen" Wörtern und ihren Varianten)

```{r}
# Liste von Varianten einlesen

words <- read.csv2("../data/raw_data/words.csv", sep = ",")

library(tidyr)

## make a columns for each variant

# 1. remove []
words$variants <- gsub("\\]", "", words$variants)
words$variants <- gsub("\\[", "", words$variants)

#2.  separate column
word_variants <- separate(data = words, col = variants, into = c("var1", "var2", "var3", "var4", "var5", "var6", "var7", "var8", "var9", "var10" ), sep = "\\|")


# mergen der Tabelle/löschen der nicht-relevanten Spalten

variants <- word_variants[,6:15]
variants <- gather(data = word_variants[,6:15], key = "nr", value = "variant" , na.rm = T)

# die Varianten rausfiltern, die NICHT in der Wortliste sind, also die eigentlichen Worte raus
list <- subset(variants, variants$variant != words$word)

# varianten aus dem df schmeißen
only_words <- ciic_words[,!(colnames(ciic_words) %in% tolower(list$variant))]  

# give rownames
rownames(only_words) <- only_words$ciic

#remove column ciic
only_words <- only_words[,-1]

# transpose
words_only <- as.data.frame(t(only_words))


```



## Korrelation von verschiedenen Wörtern miteinander

Kategorisierung der Daten notwendig, da die sehr hohen Werte (123) die eigentliche Differenzierung am Anfang überblenden.

```{r corr}

# how often which word
words_only$sum <- rowSums(words_only)

###################################### words that occur how often?
  
# too many words, therefore use only the once, that appear more than once:
words_only_many <- subset(words_only, words_only$sum > 3) # subset to words that occur more than once

######################################


# delete col with sum info
words_only_many <- words_only_many[,-ncol(words_only_many)]

#zeros to NAs as to remove na later on
words_only_many[words_only_many == 0] <- NA

#transpose 
many_only_words <- as.data.frame(t(words_only_many))

# we need the ciic column again as ID
many_only_words$ciic <- rownames(many_only_words)


library(reshape2)
# create a list of ciic to words
melted <- melt(many_only_words, ID = ciic, na.rm = T)

# crosstable to get the values for co-occurence
V <- crossprod(table(melted[1:2]))

# melt this crosstable to get a plottable dataframe
heat1 <- melt(as.matrix(V), varnames = c("row", "col"))


##categorize data

# use Jenks natural breaks 
#library(classInt)
#res <- plotJenks(heat1$value, n = 14) # to get an idea how to classify (many other possibilities) 

# categorise data
heat1$category <- cut(heat1$value, 
                   breaks=c(-Inf,0, 1, 2, 3, 4, 5, 7, 8, 10, 12, 19, 21, 33, 123, Inf), 
                   labels=c("0", "1", "2","3", "4", "5", "7","8", "10","12", "19", "21", "33", "123", 200))


#install.packages("viridis")  # Installiert, da geeignet für Farbenblinde
library("viridis")   
col <-c("black", viridis(13))

library(ggplot2)

ggplot(heat1, aes(row, col))+
  geom_tile(aes(fill = category), colour = "grey80")+
  scale_fill_manual(name = "Frequency of co-occurence", 
                    values = col,
                    breaks = c("0", "1", "2","3", "4", "5", "7","8", "10","12", "19", "21", "33", "123")) +
  theme(axis.text.x  = element_text(angle=45, vjust=1, hjust = 1))+
  xlab("")+
  ylab("")+
  labs(caption = "Words occuring more than three times in the data set")
  
ggsave("../figures/co-occurence.jpg", width = 20, height = 18, units = "cm", dpi = 300)


```






<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

##### pagebreak

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
git2r::repository(here::here())
```
