---
title: "Creating maps of ogham stones with R"
author:
  - Schmidt, Sophie
  - Thiery, Florian
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---


<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)
```

# Introduction

It is the aim of this package to use the data of the Ogi-Ogham project to create some maps and geostatistics with R.


```{r}
# load data
ogham <- read.csv2("../data/raw_data/2020-01-10_townlandsxy.csv", sep = "\t")

library(tidyr)

ogha <- separate(ogham, latlon, c("lat", "lon"), ';')

# lat lon were saved as characters, make them numbers again
ogha$lat <- as.numeric(as.character(ogha$lat))
ogha$lon <- as.numeric(as.character(ogha$lon))


##create spatialpointsdataframe
library(sp)

# transformation into Easting and Northing for percolation
coordinates(ogha) <- ~lon+lat
proj4string(ogha) <- CRS("+proj=longlat + ellps=WGS84") # This sets the projection of the data, assuming that you are using the WGS84 ellipsoid (if you do not know for sure, it is a safe assumption)

data <-spTransform(ogha,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.


ogha <- as.data.frame(coordinates(data)) # This converts the SpatialPoints object back into a data frame


# load shape
library(rgdal)

bg <- readOGR(dsn = "../data/raw_data",layer = "ne_10m_admin_0_countries")

gb_ie <- subset(bg, bg@data$ADMIN == 'United Kingdom' | bg@data$ADMIN == 'Ireland')

ie <- subset(bg, bg@data$ADMIN == 'Ireland')

```





```{r percolation}

#devtools::install_github("scschmidt/percopackage")
library("percopackage")

ogg <- cbind(ogham$ciic, ogha)

## the algorithm had problems with IDs with a "-"  -- therefore they needed to be renamed to just the first number
ogg_renamed <- ogg

library(tidyr)
# separating the IDs into two columns
ogg_renamed <- separate(data = ogg_renamed, col = PlcIndex, into = c("left", "right"), sep = "-")

#naming the columns
colnames(ogg_renamed) <- c("PlcIndex", "weg", "Easting", "Northing")

# deleting the not needed column
ogg_renamed <- ogg_renamed[,-2]

# there's a mistake there somewhere
ogg_renamed$PlcIndex[ogg_renamed$PlcIndex == "'12"] <- "12"

# percolation
percolate(data = ogg_renamed, distance_table = NULL,limit = 100, radius_unit = 1000, upper_radius = 60, lower_radius = 1, step_value = 2)

#transformation of map to same PRJ
gb_ie <- spTransform(gb_ie,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.

mapClusters(shape = gb_ie, map_name = "ogham stones in Ireland", source_file_name = "after MacAlister 1945", dpi = 300)


plotClustFreq()


```
## troubleshoot


## density Karte

```{r density plot}

library(ggplot2)

ggplot()+
      stat_density_2d(data = ogha, aes(x = lon, y = lat, fill = ..level..), alpha = 0.5, h = ogha$count, geom = "polygon")+
    scale_fill_distiller(palette="Spectral", direction=-1) +
    geom_polygon(data = gb_ie, aes(x = long, y = lat, group = group), fill = "white", alpha = 0.1, colour = "lightgrey", size = 0.5) +
  coord_equal() +
 geom_point(data = ogha, aes(x = lon, y = lat), size = 0.5) +
  theme_classic()
ggsave("../figures/densitymap.jpg", width = 20, height = 18, units = "cm", dpi = 300)

```


## wie häufig welches Wort


```{r load word data}

ciic_words <- read.csv2("../data/raw_data/ciictowords2.csv", sep = "|")
ciic_words <- ciic_words[,-c(2:5)]
ciic_words[is.na(ciic_words)] <- 0


words_ciic <- t(as.matrix(ciic_words)) # transpose = switch columns and rows
words_ciic <- as.data.frame(words_ciic)

for (i in 1:323) {
  words_ciic[,i] <- as.numeric(as.character(words_ciic[,i]))
}

colnames(words_ciic) <- ciic_words$ciic
words_ciic <- words_ciic[-1,]

words_ciic$sum <- rowSums(words_ciic)
  
words_many <- subset(words_ciic, words_ciic$sum > 3) # subset to words that occur more than three times

many_words <- as.data.frame(t(words_many))

```

## Datensatz säubern
Da sowohl Varianten von Wörtern als auch die "vollen" Wörter in der Tabelle aufgenommen wurden, muss der Datensatz bereinigt werden (ansonsten sehr hohe Korrelationen von "vollen" Wörtern und Varianten)

```{r}
# Liste von Varianten einlesen

words <- read.csv2("../data/raw_data/words.csv", sep = ",")

library(tidyr)

# remove []
words$variants <- gsub("\\]", "", words$variants)
words$variants <- gsub("\\[", "", words$variants)

word_variants <- separate(data = words, col = variants, into = c("var1", "var2", "var3", "var4", "var5", "var6", "var7", "var8", "var9", "var10" ), sep = "\\|")

# mergen der Tabelle/löschen der nicht-relevanten Spalten

variants <- word_variants[,6:15]
variants <- gather(data = word_variants[,6:15], key = "nr", value = "variant" , na.rm = T)


list <- subset(variants, variants$variant != words$word)


only_words <- ciic_words[,tolower(colnames(ciic_words)) %in% list]  
  
  
words_many <- subset(words_ciic, words_ciic$sum > 3) # subset to words that occur more than three times

many_words <- as.data.frame(t(words_many))

```



## Korrelation von verschiedenen Wörtern miteinander

Kategorisierung der Daten notwendig, da die sehr hohen Werte (168) die eigentliche Differenzierung am Anfang überblenden.

```{r corr}

many_words$ciic <- rownames(many_words)

many_words <- many_words[-324,]

many_words[many_words == 0] <- NA


library(reshape2)
melted <- melt(many_words, ID = ciic, na.rm = T)


V <- crossprod(table(melted[1:2]))

heat1 <- melt(as.matrix(V), varnames = c("row", "col"))

library(classInt)
res <- plotJenks(heat1$value, n = 12) # to get an idea how to classify (many other possibilities)

#categorize data
heat1$category <- cut(heat1$value, 
                   breaks=c(-Inf,0, 1, 3, 5, 7, 10, 15, 21, 27, 33, 123, 168, Inf), 
                   labels=c("0", "1", "2-3", "4-5", "6-7", "8-10", "11-15", "16-21", "27", "33", "123", "168", 200))

#install.packages("viridis")  # Installiert, da geeignet für Farbenblinde
library("viridis")   
col <- viridis(12)

library(ggplot2)

ggplot(heat1, aes(row, col))+
  geom_tile(aes(fill = category), colour = "grey80")+
  scale_fill_manual(name = "Frequency of co-occurence", 
                    values = col,
                    breaks = c("0", "1", "2-3", "4-5", "6-7", "8-10", "11-15", "16-21", "27", "33", "123", "168", 200)) +
  theme(axis.text.x  = element_text(angle=45, vjust=0.5))+
  xlab("")+
  ylab("")+
  labs(caption = "Words occuring more than three times in the data set")
  
ggsave("../figures/co-occurence.jpg", width = 20, height = 18, units = "cm", dpi = 300)


```






<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

##### pagebreak

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
git2r::repository(here::here())
```
