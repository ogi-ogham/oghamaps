---
title: "Playing around with ogham stones in R"
author:
  - Schmidt, Sophie
  - Thiery, Florian
  - Homburg, Timo
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---


<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)
```

# some more plots for the data paper

```{r loaddata}

d <- read.csv2("../data/raw_data/data-cluster_words.csv", sep = ",")

library(tidyr)

# lat lon were saved as characters, make them numbers again
d$x <- as.numeric(as.character(d$x))
d$y <- as.numeric(as.character(d$y))

d_nona <- subset(d, !is.na(d$x) & !is.na(d$y))
d <- d_nona

##create spatialpointsdataframe
library(sp)

# transformation into Easting and Northing for percolation
coordinates(d_nona) <- ~x+y
proj4string(d_nona) <- CRS("+proj=longlat + ellps=WGS84") # This sets the projection of the data, assuming that you are using the WGS84 ellipsoid (if you do not know for sure, it is a safe assumption)

data <-spTransform(d_nona,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.


d_df <- as.data.frame(coordinates(data)) # This converts the SpatialPoints object back into a data frame

save(d_df, file = "../data/derived_data/data-cluster_words.Rdata")

# load shape for background image
library(rgdal)

bg <- readOGR(dsn = "../data/raw_data",layer = "ne_10m_admin_0_countries")

gb_ie <- subset(bg, bg@data$ADMIN == 'United Kingdom' | bg@data$ADMIN == 'Ireland')

ie <- subset(bg, bg@data$ADMIN == 'Ireland')

gb_ie <- spTransform(gb_ie,CRS="+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") # Use whatever UTM zone you need.


```

## density Karte

```{r density plot}

library(ggplot2)
library(ggspatial)

ggplot()+
    stat_density_2d(data = d_df, aes(x = x, 
                                     y = y, 
                                     fill = ..level..), 
                    alpha = 0.75, 
                    geom = "polygon")+
    scale_fill_distiller(palette="Spectral", direction=-1,
                         name = "Dichte der Oghamsteine") +
    geom_polygon(data = gb_ie, aes(x = long, y = lat, group = group), 
                 fill = "white", 
                 alpha = 0.1, 
                 colour = "lightgrey", size = 0.5) +
    annotation_north_arrow(location = "tl", which_north = "true", 
                          pad_x = unit(0.1, "in"), pad_y = unit(0.2, "in"),
                          style = north_arrow_fancy_orienteering) +
    annotation_scale(location = "br", width_hint = 0.2) +
    geom_point(data = d_df, aes(x = x, y = y), size = 0.5, alpha = 0.5) +
    coord_equal(ylim=c(5700000,6120000), xlim = c(350000, 720000))+
    theme_classic()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))+
    scale_x_continuous(labels = scales::comma)+
    scale_y_continuous(labels = scales::comma)


ggsave("../figures/densitymap_de.jpg", width = 20, height = 18, units = "cm", dpi = 300)

```


# wie häufig welches Wort - updated version

```{r}
tbl_word_freq <- table(d$label_word)

df_word_freq <- as.data.frame(tbl_word_freq)

#filter for words used at least 20 times

df_word_20 <- subset(df_word_freq, df_word_freq$Freq >= 20)

# subset spatial dataset for only those stones with those most often used words in there:


d_d <- data.frame(d_df$x, d_df$y, d[,-c(5:7)])


d_words_20 <- subset(d_d, d_d$label_word %in% df_word_20$Var1)

library(dplyr)
library(magrittr)

for (i in d_words_20$label_word) {

  j <- subset(d_words_20, d_words_20$label_word == as.character(i))
  j <- as.data.frame(j)

  ggplot()+
    stat_density_2d(data = j, aes(x = d_df.x,
                        y = d_df.y, 
                        fill = ..level..), 
                    alpha = 0.75, 
                    geom = "polygon")+
    scale_fill_distiller(palette="Spectral", direction=-1,
                         name = paste("Dichte der Oghamsteine", i,sep = " ")) +
    geom_polygon(data = gb_ie, aes(x = long, 
                                   y = lat, group = group), 
                 fill = "white", 
                 alpha = 0.1, 
                 colour = "lightgrey", size = 0.5) +
    annotation_north_arrow(location = "tl", which_north = "true", 
                          pad_x = unit(0.1, "in"), pad_y = unit(0.2, "in"),
                          style = north_arrow_fancy_orienteering) +
    annotation_scale(location = "br", width_hint = 0.2) +
    geom_point(data = d_df, #HG: wo sind alle Oghamsteine
               aes(x = x, 
                   y = y), 
               size = 0.25, 
               alpha = 0.25,
               type = 1) +
     geom_point(data = j,  # die für dichte genutzte punkte
               aes(x = d_df.x, 
                   y = d_df.y), 
               size = 0.5, 
               alpha = 0.5,
               col = "red") +
    coord_equal(ylim=c(5700000,6120000), xlim = c(350000, 720000))+
    theme_classic()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))+
    scale_x_continuous(labels = scales::comma)+
    scale_y_continuous(labels = scales::comma)+
    labs(x="",
         y="")

ggsave(paste("../figures/density_words/dichte_",i,".jpg",sep=""), dpi = 300)
}
  
```




## Korrelation von verschiedenen Wörtern miteinander

Kategorisierung der Daten notwendig, da die sehr hohen Werte (123) die eigentliche Differenzierung am Anfang überblenden.

```{r corr}

# how often which word
words_only$sum <- rowSums(words_only, na.rm = TRUE)


###################################### words that occur how often?
  
# too many words, therefore use only the once, that appear more than once:
words_only_many <- subset(words_only, words_only$sum > 3) # subset to words that occur more than once

######################################

# Visualisierung, wie häufig die einzelnen Worte vorkommen


library("viridis")   
#col <-c("black", viridis(13))
col <-c(viridis(14))

library(ggplot2)
ggplot()+
  geom_col(data = words_only_many, 
           aes(x = reorder(rownames(words_only_many), sum),
               y = sum,
               fill = as.factor(sum)))+
    scale_fill_manual(guide=FALSE,
                    values = col[5:14],
                    breaks = c("4", "6", "7","8", "10","12", "18", "19", "33", "123")) +
  coord_flip()+
  xlab("Ogham words named more than three times")+
  ylab("count")+
  theme_bw()

ggsave("../figures/ogham_words_count.png", dpi = 300)
```


```{r}

# delete col with sum info
words_only_many <- words_only_many[,-ncol(words_only_many)]

#zeros to NAs as to remove na later on
words_only_many[words_only_many == 0] <- NA

#transpose 
many_only_words <- as.data.frame(t(words_only_many))

# we need the ciic column again as ID
many_only_words$ciic <- rownames(many_only_words)


library(reshape2)
# create a list of ciic to words
melted <- melt(many_only_words, ID = ciic, na.rm = T)

# crosstable to get the values for co-occurence
V <- crossprod(table(melted[1:2]))

# melt this crosstable to get a plottable dataframe
heat1 <- melt(as.matrix(V), varnames = c("row", "col"))


##categorize data

# use Jenks natural breaks 
#library(classInt)
#res <- plotJenks(heat1$value, n = 14) # to get an idea how to classify (many other possibilities) 

# categorise data
heat1$category <- cut(heat1$value, 
                   breaks=c(-Inf,0, 1, 2, 3, 4, 5, 7, 8, 10, 12, 19, 21, 33, 123, Inf), 
                   labels=c("0", "1", "2","3", "4", "5", "7","8", "10","12", "19", "21", "33", "123", 200))


#install.packages("viridis")  # Installiert, da geeignet für Farbenblinde
library("viridis")   
#col <-c("black", viridis(13))
col <-c(viridis(14))


library(ggplot2)

ggplot(heat1, aes(row, col))+
  geom_tile(aes(fill = category), colour = "grey80")+
  geom_text(aes(label=category, colour = "white"), size = 2)+
  scale_fill_manual(name = "Frequency of co-occurence", 
                    values = col,
                    breaks = c("0", "1", "2","3", "4", "5", "7","8", "10","12", "19", "21", "33", "123")) +
      scale_colour_manual(values="white", guide=FALSE)+
  theme(axis.text.x  = element_text(angle=45, vjust=1, hjust = 1))+
  xlab("")+
  ylab("")+
  labs(caption = "Words occuring more than three times in the data set")
  
ggsave("../figures/co-occurence.jpg", width = 20, height = 18, units = "cm", dpi = 300)


```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

##### pagebreak

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
git2r::repository(here::here())
```
